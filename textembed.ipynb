{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Downloading numpy-2.1.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.8/60.8 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ahame\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (24.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.5/41.5 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-18.0.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.10.10-cp311-cp311-win_amd64.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ahame\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ahame\\appdata\\roaming\\python\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.8/60.8 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.17.1-cp311-cp311-win_amd64.whl.metadata (66 kB)\n",
      "     ---------------------------------------- 0.0/66.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 66.9/66.9 kB 1.8 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.4.0-cp311-cp311-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ahame\\appdata\\roaming\\python\\python311\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ahame\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets)\n",
      "  Downloading propcache-0.2.0-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Downloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/10.0 MB 9.8 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 1.0/10.0 MB 10.5 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 1.4/10.0 MB 10.0 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.9/10.0 MB 9.9 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.4/10.0 MB 10.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.9/10.0 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.5/10.0 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.1/10.0 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.6/10.0 MB 10.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.1/10.0 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.6/10.0 MB 10.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.0/10.0 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.5/10.0 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.0/10.0 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.5/10.0 MB 10.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.1/10.0 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.7/10.0 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.2/10.0 MB 10.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 10.5 MB/s eta 0:00:00\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "   ---------------------------------------- 0.0/480.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 480.6/480.6 kB 10.0 MB/s eta 0:00:00\n",
      "Downloading torch-2.5.1-cp311-cp311-win_amd64.whl (203.1 MB)\n",
      "   ---------------------------------------- 0.0/203.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/203.1 MB 14.2 MB/s eta 0:00:15\n",
      "   ---------------------------------------- 0.9/203.1 MB 11.6 MB/s eta 0:00:18\n",
      "   ---------------------------------------- 1.4/203.1 MB 11.4 MB/s eta 0:00:18\n",
      "   ---------------------------------------- 2.0/203.1 MB 11.4 MB/s eta 0:00:18\n",
      "   ---------------------------------------- 2.4/203.1 MB 10.2 MB/s eta 0:00:20\n",
      "    --------------------------------------- 2.9/203.1 MB 10.9 MB/s eta 0:00:19\n",
      "    --------------------------------------- 3.5/203.1 MB 11.1 MB/s eta 0:00:18\n",
      "    --------------------------------------- 4.0/203.1 MB 11.2 MB/s eta 0:00:18\n",
      "    --------------------------------------- 4.5/203.1 MB 10.7 MB/s eta 0:00:19\n",
      "    --------------------------------------- 5.1/203.1 MB 10.8 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 5.5/203.1 MB 10.7 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 6.0/203.1 MB 10.4 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 6.5/203.1 MB 10.5 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 7.1/203.1 MB 10.9 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 7.8/203.1 MB 10.8 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 8.4/203.1 MB 10.9 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 9.0/203.1 MB 11.0 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 9.7/203.1 MB 11.2 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 10.4/203.1 MB 11.1 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 11.1/203.1 MB 11.5 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 11.8/203.1 MB 11.7 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 12.5/203.1 MB 11.9 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 13.3/203.1 MB 12.3 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 14.0/203.1 MB 12.6 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 14.8/203.1 MB 12.8 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 15.5/203.1 MB 13.1 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 16.2/203.1 MB 13.6 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 17.1/203.1 MB 13.9 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 18.0/203.1 MB 14.9 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 18.7/203.1 MB 15.2 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 19.1/203.1 MB 15.2 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 19.1/203.1 MB 14.2 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 19.2/203.1 MB 13.4 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 19.4/203.1 MB 12.6 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 19.6/203.1 MB 11.9 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 19.9/203.1 MB 11.5 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 20.7/203.1 MB 11.7 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 21.6/203.1 MB 11.9 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 22.4/203.1 MB 12.1 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 23.4/203.1 MB 12.1 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 24.4/203.1 MB 12.6 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 25.4/203.1 MB 12.8 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 26.4/203.1 MB 13.1 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 27.5/203.1 MB 13.1 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 28.6/203.1 MB 13.6 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 29.7/203.1 MB 18.2 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 31.0/203.1 MB 21.1 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 32.2/203.1 MB 22.6 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 33.4/203.1 MB 21.9 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 34.5/203.1 MB 22.6 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 35.4/203.1 MB 21.8 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 36.3/203.1 MB 21.8 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 37.2/203.1 MB 21.8 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 38.1/203.1 MB 21.1 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 39.2/203.1 MB 21.1 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 39.8/203.1 MB 21.1 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 40.8/203.1 MB 20.5 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 41.9/203.1 MB 20.5 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 42.9/203.1 MB 20.5 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 44.1/203.1 MB 19.8 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 45.2/203.1 MB 19.8 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 46.3/203.1 MB 21.1 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 47.4/203.1 MB 21.1 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 48.4/203.1 MB 21.1 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 49.6/203.1 MB 21.8 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 50.7/203.1 MB 21.8 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 51.5/203.1 MB 21.9 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 52.7/203.1 MB 22.6 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 54.1/203.1 MB 22.6 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 55.2/203.1 MB 23.4 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 56.5/203.1 MB 23.4 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 57.6/203.1 MB 23.4 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 58.8/203.1 MB 24.2 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 60.1/203.1 MB 25.1 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 61.0/203.1 MB 24.3 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 62.3/203.1 MB 25.2 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 63.6/203.1 MB 25.2 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 64.6/203.1 MB 25.2 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 65.9/203.1 MB 26.2 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 67.4/203.1 MB 26.2 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 68.8/203.1 MB 27.3 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 70.2/203.1 MB 27.3 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 71.7/203.1 MB 28.5 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 73.0/203.1 MB 28.5 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 74.6/203.1 MB 29.7 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 76.2/203.1 MB 28.4 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 77.7/203.1 MB 29.7 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 79.3/203.1 MB 31.2 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 80.8/203.1 MB 28.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 82.0/203.1 MB 29.7 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 83.3/203.1 MB 29.7 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 84.5/203.1 MB 28.4 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 85.7/203.1 MB 27.3 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 87.0/203.1 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 88.3/203.1 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 89.6/203.1 MB 25.1 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 90.2/203.1 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 91.2/203.1 MB 23.4 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 92.1/203.1 MB 22.6 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 93.3/203.1 MB 22.6 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 94.7/203.1 MB 23.4 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 95.7/203.1 MB 24.2 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 97.0/203.1 MB 24.2 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 98.4/203.1 MB 24.2 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 99.5/203.1 MB 24.2 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 101.0/203.1 MB 27.3 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 102.4/203.1 MB 27.3 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 103.9/203.1 MB 27.3 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 105.4/203.1 MB 28.5 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 106.6/203.1 MB 26.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 107.7/203.1 MB 27.3 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 109.2/203.1 MB 28.4 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 110.6/203.1 MB 28.4 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 112.1/203.1 MB 28.5 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 113.6/203.1 MB 29.8 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 115.3/203.1 MB 31.2 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 116.8/203.1 MB 31.2 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 118.4/203.1 MB 29.7 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 120.0/203.1 MB 31.2 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 121.3/203.1 MB 31.2 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 122.7/203.1 MB 29.7 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 124.1/203.1 MB 31.2 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 125.7/203.1 MB 31.2 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 126.9/203.1 MB 31.2 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 128.8/203.1 MB 31.2 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 130.0/203.1 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 132.3/203.1 MB 34.6 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 133.9/203.1 MB 32.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 135.7/203.1 MB 32.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 137.4/203.1 MB 36.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 138.9/203.1 MB 32.8 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 140.8/203.1 MB 34.4 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 142.2/203.1 MB 34.4 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 143.8/203.1 MB 34.4 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 145.4/203.1 MB 34.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 147.0/203.1 MB 34.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 148.6/203.1 MB 36.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 150.5/203.1 MB 34.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 152.2/203.1 MB 34.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 154.1/203.1 MB 34.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 155.9/203.1 MB 36.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 157.3/203.1 MB 36.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 159.0/203.1 MB 36.3 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 160.7/203.1 MB 36.3 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 162.5/203.1 MB 36.4 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 163.8/203.1 MB 36.4 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 165.0/203.1 MB 36.3 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 166.3/203.1 MB 34.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 168.0/203.1 MB 34.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 169.8/203.1 MB 34.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 171.8/203.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 173.1/203.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 175.2/203.1 MB 38.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 177.2/203.1 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 179.3/203.1 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 181.5/203.1 MB 43.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 183.4/203.1 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 184.7/203.1 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 186.3/203.1 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 188.0/203.1 MB 38.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 189.7/203.1 MB 36.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 191.3/203.1 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 193.2/203.1 MB 36.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 194.6/203.1 MB 32.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 196.0/203.1 MB 32.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 197.7/203.1 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.4/203.1 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  201.0/203.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.5/203.1 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.1/203.1 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.1/203.1 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.1/203.1 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.1/203.1 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 203.1/203.1 MB 20.5 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 1.5/6.2 MB 49.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.2/6.2 MB 33.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.9/6.2 MB 34.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 30.4 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.6/78.6 kB ? eta 0:00:00\n",
      "Downloading scikit_learn-1.5.2-cp311-cp311-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.6/11.0 MB 52.2 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.1/11.0 MB 32.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.8/11.0 MB 33.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.1/11.0 MB 32.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.7/11.0 MB 33.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.8/11.0 MB 31.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.5/11.0 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 29.7 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.3/116.3 kB 6.6 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "   ---------------------------------------- 0.0/179.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 179.3/179.3 kB ? eta 0:00:00\n",
      "Downloading aiohttp-3.10.10-cp311-cp311-win_amd64.whl (381 kB)\n",
      "   ---------------------------------------- 0.0/381.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 381.6/381.6 kB 24.8 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "   ---------------------------------------- 0.0/447.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 447.5/447.5 kB 27.3 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 301.8/301.8 kB 18.2 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "   ---------------------------------------- 0.0/143.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 143.5/143.5 kB 8.3 MB/s eta 0:00:00\n",
      "Downloading numpy-2.1.3-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.9/12.9 MB 61.4 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.7/12.9 MB 47.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.5/12.9 MB 38.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.6/12.9 MB 34.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.9/12.9 MB 31.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 9.0/12.9 MB 30.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.9/12.9 MB 28.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.9/12.9 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.8/12.9 MB 25.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.9/12.9 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 22.6 MB/s eta 0:00:00\n",
      "Downloading pyarrow-18.0.0-cp311-cp311-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/25.1 MB 32.3 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 2.3/25.1 MB 23.9 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 3.4/25.1 MB 24.3 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 4.2/25.1 MB 24.3 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 4.2/25.1 MB 24.3 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 4.2/25.1 MB 24.3 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 4.6/25.1 MB 13.5 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 5.7/25.1 MB 14.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 6.6/25.1 MB 15.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 7.5/25.1 MB 15.1 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 8.2/25.1 MB 15.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 9.1/25.1 MB 15.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 9.4/25.1 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 9.4/25.1 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 9.6/25.1 MB 13.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 10.6/25.1 MB 13.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 11.6/25.1 MB 13.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 12.6/25.1 MB 13.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 13.6/25.1 MB 13.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 14.8/25.1 MB 15.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 15.9/25.1 MB 16.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 16.8/25.1 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 17.8/25.1 MB 16.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.7/25.1 MB 16.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.9/25.1 MB 16.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.9/25.1 MB 16.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.9/25.1 MB 16.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.8/25.1 MB 16.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.0/25.1 MB 16.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.0/25.1 MB 16.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.0/25.1 MB 16.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.0/25.1 MB 16.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.4/25.1 MB 13.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.6/25.1 MB 13.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 4.5 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
      "   ---------------------------------------- 0.0/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 153.6/162.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 162.0/162.0 kB 68.9 kB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "   ---------------------------------------- 0.0/274.1 kB ? eta -:--:--\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.2/274.1 kB 16.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- 274.1/274.1 kB 157.8 kB/s eta 0:00:00\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 61.4/64.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.9/64.9 kB 48.6 kB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.5-cp311-none-win_amd64.whl (285 kB)\n",
      "   ---------------------------------------- 0.0/286.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 286.0/286.0 kB 8.9 MB/s eta 0:00:00\n",
      "Downloading scipy-1.14.1-cp311-cp311-win_amd64.whl (44.8 MB)\n",
      "   ---------------------------------------- 0.0/44.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.0/44.8 MB 32.7 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 2.2/44.8 MB 28.4 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 3.3/44.8 MB 26.2 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 4.5/44.8 MB 25.9 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 5.8/44.8 MB 26.5 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 6.7/44.8 MB 25.2 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 7.5/44.8 MB 23.9 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 8.2/44.8 MB 22.9 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 9.1/44.8 MB 22.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 10.1/44.8 MB 22.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 11.2/44.8 MB 21.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 12.4/44.8 MB 21.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 13.5/44.8 MB 21.1 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 14.6/44.8 MB 20.5 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 15.6/44.8 MB 20.5 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 16.8/44.8 MB 21.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 16.8/44.8 MB 21.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 16.8/44.8 MB 21.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 16.8/44.8 MB 21.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 17.8/44.8 MB 16.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 19.0/44.8 MB 17.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 20.1/44.8 MB 17.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 21.2/44.8 MB 17.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 22.5/44.8 MB 17.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 23.8/44.8 MB 17.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 25.0/44.8 MB 18.2 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 26.1/44.8 MB 18.2 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 27.3/44.8 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 28.6/44.8 MB 25.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 29.0/44.8 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 29.0/44.8 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 29.0/44.8 MB 25.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 29.9/44.8 MB 19.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 31.2/44.8 MB 19.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 31.8/44.8 MB 19.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 31.8/44.8 MB 19.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 32.1/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 33.3/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 34.4/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 34.8/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 34.9/44.8 MB 14.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 36.0/44.8 MB 14.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 37.3/44.8 MB 14.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 37.7/44.8 MB 13.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 38.1/44.8 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 39.0/44.8 MB 13.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 40.3/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 40.8/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 41.1/44.8 MB 15.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 42.4/44.8 MB 17.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 43.3/44.8 MB 17.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.4/44.8 MB 17.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.4/44.8 MB 17.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.8/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.8/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.8/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.8/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.8/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.8/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.8/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.8/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.8/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.8/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.8/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.8/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.8/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.8/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.8/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.8/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.8/44.8 MB 7.5 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.20.3-cp311-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 1.1/2.4 MB 36.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.2/2.4 MB 35.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 30.4 MB/s eta 0:00:00\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.3/133.3 kB ? eta 0:00:00\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 1.3/1.7 MB 27.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 22.0 MB/s eta 0:00:00\n",
      "Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/11.6 MB 39.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.0/11.6 MB 38.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.0/11.6 MB 38.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.0/11.6 MB 38.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.8/11.6 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.5/11.6 MB 19.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.3/11.6 MB 20.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.3/11.6 MB 20.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.5/11.6 MB 14.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.2/11.6 MB 14.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.3/11.6 MB 15.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.6/11.6 MB 15.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.6/11.6 MB 15.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.6/11.6 MB 15.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.2/11.6 MB 13.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.4/11.6 MB 13.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 13.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 13.1 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "   ---------------------------------------- 0.0/63.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 63.0/63.0 kB ? eta 0:00:00\n",
      "Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "   ---------------------------------------- 0.0/167.3 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 41.0/167.3 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 41.0/167.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 167.3/167.3 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.4.0-cp311-cp311-win_amd64.whl (101 kB)\n",
      "   ---------------------------------------- 0.0/101.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 101.8/101.8 kB 5.7 MB/s eta 0:00:00\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl (51 kB)\n",
      "   ---------------------------------------- 0.0/51.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 51.6/51.6 kB ? eta 0:00:00\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "   ---------------------------------------- 0.0/70.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 70.4/70.4 kB ? eta 0:00:00\n",
      "Downloading MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 536.2/536.2 kB 17.0 MB/s eta 0:00:00\n",
      "Downloading multidict-6.1.0-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "   ---------------------------------------- 0.0/508.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 508.0/508.0 kB 16.1 MB/s eta 0:00:00\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "   ---------------------------------------- 0.0/346.6 kB ? eta -:--:--\n",
      "   -------------------------------- ------ 286.7/346.6 kB 18.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 286.7/346.6 kB 18.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 337.9/346.6 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 346.6/346.6 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "   ---------------------------------------- 0.0/126.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 126.3/126.3 kB 7.3 MB/s eta 0:00:00\n",
      "Downloading yarl-1.17.1-cp311-cp311-win_amd64.whl (90 kB)\n",
      "   ---------------------------------------- 0.0/90.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 90.2/90.2 kB 5.3 MB/s eta 0:00:00\n",
      "Downloading propcache-0.2.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.9/44.9 kB 2.2 MB/s eta 0:00:00\n",
      "Installing collected packages: pytz, mpmath, xxhash, urllib3, tzdata, tqdm, threadpoolctl, sympy, safetensors, regex, pyyaml, pyarrow, propcache, numpy, networkx, multidict, MarkupSafe, joblib, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, aiohappyeyeballs, yarl, scipy, requests, pandas, multiprocess, jinja2, aiosignal, torch, scikit-learn, huggingface-hub, aiohttp, tokenizers, transformers, datasets\n",
      "Successfully installed MarkupSafe-3.0.2 aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 attrs-24.2.0 certifi-2024.8.30 charset-normalizer-3.4.0 datasets-3.1.0 dill-0.3.8 filelock-3.16.1 frozenlist-1.5.0 fsspec-2024.9.0 huggingface-hub-0.26.2 idna-3.10 jinja2-3.1.4 joblib-1.4.2 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 networkx-3.4.2 numpy-2.1.3 pandas-2.2.3 propcache-0.2.0 pyarrow-18.0.0 pytz-2024.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 sympy-1.13.1 threadpoolctl-3.5.0 tokenizers-0.20.3 torch-2.5.1 tqdm-4.67.0 transformers-4.46.2 tzdata-2024.2 urllib3-2.2.3 xxhash-3.5.0 yarl-1.17.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets torch tqdm scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\ahame\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (24.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertModel(\n",
      "  (embeddings): Embeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (layer): ModuleList(\n",
      "      (0-5): 6 x TransformerBlock(\n",
      "        (attention): DistilBertSdpaAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained DistilBERT model and tokenizer\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertModel.from_pretrained(model_name)\n",
    "\n",
    "# Check the model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 120000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 7600\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a dataset from Hugging Face datasets library\n",
    "dataset = load_dataset('ag_news')\n",
    "\n",
    "# View dataset structure\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# Tokenize the train and test splits\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format to PyTorch tensors\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load a pre-trained DistilBERT model with a classification head\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate==0.26.0 in c:\\users\\ahame\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ahame\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate==0.26.0) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ahame\\appdata\\roaming\\python\\python311\\site-packages (from accelerate==0.26.0) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\ahame\\appdata\\roaming\\python\\python311\\site-packages (from accelerate==0.26.0) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ahame\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate==0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\ahame\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate==0.26.0) (2.5.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\ahame\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate==0.26.0) (0.26.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\ahame\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate==0.26.0) (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\ahame\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ahame\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\ahame\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ahame\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ahame\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ahame\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate==0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ahame\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.26.0) (1.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ahame\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub->accelerate==0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\ahame\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub->accelerate==0.26.0) (4.67.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ahame\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate==0.26.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ahame\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate==0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ahame\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.26.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ahame\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ahame\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.26.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ahame\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.26.0) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate==0.26.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 44/22500 [1:23:52<713:23:35, 114.37s/it]\n",
      "  0%|          | 54/22500 [06:34<41:55:19,  6.72s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 19\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Initialize the Trainer\u001b[39;00m\n\u001b[0;32m     12\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     13\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                        \n\u001b[0;32m     14\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                  \n\u001b[0;32m     15\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],  \n\u001b[0;32m     16\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m],    \n\u001b[0;32m     17\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ahame\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ahame\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2479\u001b[0m )\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2487\u001b[0m ):\n\u001b[0;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\ahame\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3585\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\ahame\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ahame\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ahame\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ahame\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:978\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    976\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 978\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    987\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[0;32m    988\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ahame\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ahame\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ahame\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:798\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[0;32m    794\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_attention_mask_for_sdpa(\n\u001b[0;32m    795\u001b[0m             attention_mask, embeddings\u001b[38;5;241m.\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    796\u001b[0m         )\n\u001b[1;32m--> 798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ahame\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ahame\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ahame\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:551\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    543\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    544\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    545\u001b[0m         hidden_state,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    548\u001b[0m         output_attentions,\n\u001b[0;32m    549\u001b[0m     )\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 551\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\ahame\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ahame\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ahame\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:477\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    486\u001b[0m     sa_output, sa_weights \u001b[38;5;241m=\u001b[39m sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ahame\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ahame\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ahame\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:403\u001b[0m, in \u001b[0;36mDistilBertSdpaAttention.forward\u001b[1;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    400\u001b[0m     k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    401\u001b[0m     v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m--> 403\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m unshape(attn_output)\n\u001b[0;32m    413\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_lin(attn_output)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    evaluation_strategy=\"epoch\",     \n",
    "    learning_rate=2e-5,              \n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=64,   \n",
    "    num_train_epochs=3,              \n",
    "    weight_decay=0.01,               \n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                        \n",
    "    args=training_args,                  \n",
    "    train_dataset=tokenized_datasets['train'],  \n",
    "    eval_dataset=tokenized_datasets['test'],    \n",
    ")\n",
    "\n",
    "trainer.train()\n",
# Evaluate the model on the test dataset
eval_results = trainer.evaluate()

# Print evaluation results
print(eval_results)

model.save_pretrained("./fine_tuned_model")
tokenizer.save_pretrained("./fine_tuned_model")

def get_embeddings(texts):
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')
    with torch.no_grad():
        outputs = model(**inputs)
    embeddings = outputs.last_hidden_state.mean(dim=1)  # Average the token embeddings
    return embeddings

# Example usage
texts = ["This is a test sentence.", "Fine-tuning a model is fun.", "White evangelical voters show steadfast support for Donald Trump’s presidency"]
embeddings = get_embeddings(texts)
print(embeddings)
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
